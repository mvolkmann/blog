<link rel="preload" as="style" href="/blog/assets/prism-a11y.css"><link rel="stylesheet" href="/blog/assets/prism-a11y.css"><link rel="stylesheet" href="/blog/assets/topic.css"><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><h2>PyTorch library</h2><aside><nav class="toc"><ol><li><a href="#overview">Overview</a></li><li><a href="#installing-pytorch">Installing PyTorch</a></li><li><a href="#using-pytorch-from-jupyter-lab">Using PyTorch from Jupyter Lab</a></li><li><a href="#using-a-pre-trained-model">Using a pre-trained model</a></li><li><a href="#data-sources">Data sources</a></li><li><a href="#image-recognition">Image recognition</a></li><li><a href="#conventions">Conventions</a></li><li><a href="#generative-adversarial-network-(gan)">Generative Adversarial Network (GAN)</a></li><li><a href="#scene-classification">Scene Classification</a></li><li><a href="#torch-hub">Torch Hub</a></li></ol></nav></aside><article><h2 id="overview" tabindex="-1">Overview</h2><p><a href="https://pytorch.org/?v=1.0.21" rel="noopener" target="_blank">PyTorch</a> is a deep learning library for Python. It is a port of <a href="http://torch.ch/?v=1.0.21" rel="noopener" target="_blank">Torch</a> which is implemented in C and Lua. Many PyTorch operations are primarily implemented in C++ and <a href="https://developer.nvidia.com/cuda-zone?v=1.0.21" rel="noopener" target="_blank">CUDA</a>. CUDA is a language created by NVIDIA that is similar to C++ and supports massive parallelism on graphical processing units (GPUs). Both are frameworks for implementing deep learning algorithms. The first release of PyTorch was in January, 2017.</p><p>Deep learning is a subcategory of artificial intelligence (AI). It involves training neural networks using large amounts of data referred to as a &quot;training set&quot;. The result is a function that accepts input similar to the training set data and outputs something about it. A classic example is taking a photo of a dog and identifying the breed.</p><p>Many applications of deep learning involve image recognition. Other examples include bioinformatics, customer relationship management, demographic predictions fraud detection, game playing, natural language processing, recommendation systems, and speech recognition</p><p>Data is supplied using NumPy data structures.</p><p>&quot;Feature engineering&quot; involves manually determining significant input features to measure, implementing their detection and measurement, and writing an algorithm to combine feature measurements in order to classify inputs. Contrast this to deep learning where feature detection, measurement, and combining measurements is done automatically through training on large sets of inputs. These two approaches, feature engineering and deep learning, can be combined.</p><p>Deep learning typically computes a numerical score for a set of inputs (such as the pixels of an image) and determines the difference between that score and the expected score. Training serves to incrementally lower these differences.</p><p>Currently the most popular alternative to PyTorch is TensorFlow. The code written to use PyTorch tends to be more &quot;pythonic&quot; (idiomatic Python). PyTorch is also regarded as easier to learn than TensorFlow. Their features sets overlap significantly.</p><p>PyTorch provides the &quot;tensor&quot; data structure which is a multidimensional array similar to arrays in NumPy. And just like NumPy, PyTorch implements highly optimized operations on this data structure, that have an API similar to NumPy. This operations are especially fast when run on graphical processing units (GPUs), often providing a speed improvement in the neighborhood of 50 times.</p><p>PyTorch provides a <code>DataLoader</code> class that can load data in the background in preparation for use by the training loop. Each iteration of the training loop evaluates the current model using data from the <code>DataLoader</code>. Model outputs are compared to target outputs using a loss function. The PyTorch &quot;autograd&quot; engine then modifies the model in order to produce outputs that are closer to targets.</p><p>TorchScript can be used to compile models ahead of time. This results in a set of instructions that can be executed in environments that do not use Python such as C++ applications or mobile devices.</p><p>Full training for complex models and large datasets typically require access to a CUDA-capable GPU in order to complete in a reasonable amount of time (hours versus days). Note that the GPUs in current Apple laptops do not support CUDA and precompiled versions of PyTorch for macOS only utilize CPUs. Currently, CUDA support on macOS is only available by building PyTorch from source</p><p>Some cloud platforms provide online access to Jupyter Notebooks that have PyTorch preinstalled and can process code using GPUs. One example is <a href="https://colab.research.google.com?v=1.0.21" rel="noopener" target="_blank">Colabortory</a>.</p><h2 id="installing-pytorch" tabindex="-1">Installing PyTorch</h2><p>To install PyTorch using pip, enter <code>pip install torch torchvision</code>.</p><p>To install PyTorch using Anaconda, enter <code>conda install pytorch torchvision -c pytorch</code></p><p>To get the version of PyTorch that is installed, run the following code:</p><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> torch<br><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span></code></pre><h2 id="using-pytorch-from-jupyter-lab" tabindex="-1">Using PyTorch from Jupyter Lab</h2><ol><li>Run the Anaconda-Navigator app. (For details on installing this, see <a href="/blog/python/anaconda/">here</a>.)</li><li>Click the &quot;Launch&quot; button for JupyterLab.</li><li>It will open in a new tab in your default web browser.</li><li>In a cell of a notebook, enter the code above to determine the version of PyTorch available from the notebook.</li><li>Press shift-enter to execute the cell.</li></ol><h2 id="using-a-pre-trained-model" tabindex="-1">Using a pre-trained model</h2><p>Creating a deep learning model is very time consuming. A large amount of training data must be acquired and prepared. A model must be designed. Training the model can take many hours.</p><p>Another option it to use a pre-trained model created by someone else. <a href="https://pytorch.org/hub/?v=1.0.21" rel="noopener" target="_blank">PyTorch Hub</a> provides some of these.</p><p>Another source is the <a href="http://github.com/pytorch/vision?v=1.0.21" rel="noopener" target="_blank">TorchVision</a> project which contains a collection of notable neural network models for computer vision.</p><p>To see the available models in TorchVision:</p><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> models<br><span class="token builtin">dir</span><span class="token punctuation">(</span>models<span class="token punctuation">)</span></code></pre><p>This outputs a list of names. Names that start with an uppercase letter represent Python model classes. Names that start with a lowercase letter represent Python functions that can be called to instantiate a model using one of the model classes. For example, model classes include <code>AlexNet</code>, <code>Inception3</code>, and <code>ResNet</code>. <a href="http://mng.bz/lo6z?v=1.0.21" rel="noopener" target="_blank">AlexNet</a>, <a href="http://arxiv.org/pdf/1512.00567.pdf?v=1.0.21" rel="noopener" target="_blank">Inception3</a>, and <a href="http://arxiv.org/pdf/1512.03385.pdf?v=1.0.21" rel="noopener" target="_blank">ResNet</a>. Functions include <code>alexnet</code>, <code>inception_v3</code>, and <code>resnet101</code>.</p><p>The ImageNet Large Scale Visual Recognition Competition (ILSVRC) is an annual competition that began in 2010. Tasks to be solved vary each year. Example tasks include:</p><ul><li>image classification: Identify the categories of objects that are present.</li><li>object localization: Report the position of an object.</li><li>object detection: List and label the objects that are present.</li><li>scene classification: Classify a depicted situation.</li><li>scene parsing: Segment an image into regions that match object categories.</li></ul><p>AlexNet won an ILSVRC award in 2012 with a top-5 test error rate of 15.4% which means that it failed to include the correct label in its top five predictions only 15.4% of the time. In 2020 top-5 error rates can be 3% or lower.</p><p>RestNet is a residual network that won an ILSVRC award in 2015. It was trained using 1.2 million images in the ImageNet dataset to classify images into 1000 categories. This means it can only label images using those 1000 categories.</p><p>Here is code to perform image recognition using a ResNet 101-layer convolutional neural network:</p><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<br><span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<br><span class="token keyword">import</span> torch<br><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> models<span class="token punctuation">,</span> transforms<br><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<br><br><span class="token comment"># Create an instance of a neural network.</span><br><span class="token comment"># The 1000 category names this model supports are listed in</span><br><span class="token comment"># the file imagenet_classes.txt file that is read in later.</span><br><span class="token comment"># The first time function is called, it downloads the model</span><br><span class="token comment"># to the file $TORCH_HOME/checkpoints/resnet101-5d3b4d8f.pth.</span><br><span class="token comment"># In macOS, TORCH_HOME defaults to ~/.cache.torch.</span><br><span class="token comment"># Download progress will be displayed.</span><br><span class="token comment"># For me it took 13 seconds to complete the download.</span><br>resnet <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet101<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br><br><span class="token comment"># Put the network into "eval" mode because we want to</span><br><span class="token comment"># evaluate input rather than perform training.</span><br>resnet<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span><br><br><span class="token comment"># Load an image.</span><br>img_path <span class="token operator">=</span> <span class="token string">'my-dog.jpg'</span><br>img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./'</span> <span class="token operator">+</span> img_path<span class="token punctuation">)</span><br><br><span class="token comment"># Open the image in the default image app (ex. Preview).</span><br><span class="token comment">#img.show()</span><br><br><span class="token comment"># Prepare the image for input to the network using the same</span><br><span class="token comment"># preprocessing steps that were applied to images during training.</span><br><span class="token comment"># One way to discover the preprocessing expected by a given model</span><br><span class="token comment"># is to find its documentation at https://pytorch.org/hub.</span><br><span class="token comment"># Click the magnifying glass icon in the upper-right</span><br><span class="token comment"># to search for a model such as ResNet.</span><br><span class="token comment"># For example, documentation on ResNet can be found at</span><br><span class="token comment"># https://pytorch.org/hub/pytorch_vision_fcn_resnet101/.</span><br>preprocess <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    <span class="token comment"># Resize the image to reduce the number of pixels to be</span><br>    <span class="token comment"># processed and match the image sizes used for training.</span><br>    transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br><br>    <span class="token comment"># Crop the image to a smaller size about its center,</span><br>    <span class="token comment"># removing unnecessary pixels at the edges.</span><br>    transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br><br>    <span class="token comment"># Convert the image data to a tensor object.</span><br>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br><br>    <span class="token comment"># Normalize the red/green/blue values of the pixels to</span><br>    <span class="token comment"># have the same mean and standard deviation values</span><br>    <span class="token comment"># that were used when the model was trained.</span><br>   transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><br>       mean<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># [red, green, blue]</span><br>       std<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">]</span> <span class="token comment"># [red, green, blue]</span><br>   <span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span><br>img_t <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>img<span class="token punctuation">)</span><br><span class="token comment">#print(img_t.size()) # torch.Size([3, 224, 224])</span><br><br><span class="token comment"># Render the preprocessed image.</span><br><span class="token comment"># This will have very odd coloring!</span><br><span class="token comment">#img_pil = tensor_to_pil(img_t)</span><br><span class="token comment">#plt.imshow(img_pil)</span><br><span class="token comment">#plt.show()</span><br><span class="token comment">#img_pil.save('my-dog-preprocessed.png')</span><br><br><span class="token comment"># Create a 1D tensor object from the image</span><br><span class="token comment"># with the data starting at index zero.</span><br><span class="token comment"># Why isn't zero the default?</span><br>batch_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>img_t<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><br><span class="token comment">#print(batch_t.size()) # torch.Size([1, 3, 224, 224])</span><br><br><span class="token comment"># Perform inference to get predicted classes.</span><br><span class="token comment"># "out" is set to a tensor that contains percentage predictions</span><br><span class="token comment"># for each of the 1000 possible labels.</span><br>out <span class="token operator">=</span> resnet<span class="token punctuation">(</span>batch_t<span class="token punctuation">)</span><br><span class="token comment">#print('out size =', out.size()) # torch.Size([1, 1000])</span><br><br><span class="token comment"># Get the 1000 possible labels from a text file.</span><br><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./dlwpt-code-master/data/p1ch2/imagenet_classes.txt'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><br>    labels <span class="token operator">=</span> <span class="token punctuation">[</span>line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><br><br><span class="token comment"># Find the index with the highest score.</span><br><span class="token comment"># _ is set to the highest value, but we don't need that.</span><br>_<span class="token punctuation">,</span> index <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>out<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><br><span class="token comment"># index will be a 1D tensor containing one integer.</span><br><span class="token comment"># We can also get the tensor value at this index with</span><br><span class="token comment"># out.detach().numpy()[0][index])</span><br><br><span class="token comment"># Compute values that are like percentage certainty from each tensor value.</span><br>percentages <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>out<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><br><br><span class="token comment"># Output the label and percentage associated</span><br><span class="token comment"># with the index of the highest score.</span><br>i <span class="token operator">=</span> index<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br>label <span class="token operator">=</span> labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><br>pct <span class="token operator">=</span> percentages<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><br><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'I am </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">round</span><span class="token punctuation">(</span>pct<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">% sure this is a </span><span class="token interpolation"><span class="token punctuation">{</span>label<span class="token punctuation">}</span></span><span class="token string">.\n'</span></span><span class="token punctuation">)</span><br><br><span class="token comment"># Output the top predictions.</span><br>_<span class="token punctuation">,</span> indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>out<span class="token punctuation">,</span> descending<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br>n <span class="token operator">=</span> <span class="token number">5</span><br><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'My top'</span><span class="token punctuation">,</span> n<span class="token punctuation">,</span> <span class="token string">'predictions are:'</span><span class="token punctuation">)</span><br><span class="token keyword">for</span> i <span class="token keyword">in</span> indices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">:</span><br>    label <span class="token operator">=</span> labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><br>    pct <span class="token operator">=</span> percentages<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><br>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{</span>label<span class="token punctuation">}</span></span><span class="token string"> - </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">round</span><span class="token punctuation">(</span>pct<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span></code></pre><p>This code requires the following installs:</p><pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> matplotlib <span class="token comment"># for rendering images</span><br>pip <span class="token function">install</span> pillow <span class="token comment"># for loading images</span><br>pip <span class="token function">install</span> torch torchvision <span class="token comment"># for neural networks</span></code></pre><p>Here is sample output from the code above:</p><pre class="language-text"><code class="language-text">I am 95.97% sure this is a whippet.<br><br>My top 5 predictions are:<br>whippet - 95.97%<br>Italian greyhound - 2.44%<br>basenji - 0.59%<br>Ibizan hound, Ibizan Podenco - 0.52%<br>toy terrier - 0.15%</code></pre><p>The input image used is:</p><p><img alt="input image" class="keep-size" src="/blog/assets/pytorch-input.jpg?v=1.0.21"></p><p>The image that results from preprocessing is:</p><p><img alt="preprocessed image" class="keep-size" src="/blog/assets/pytorch-input-preprocessed.png?v=1.0.21"></p><p>Note that if an image is fed into the model that does not match any of the 1000 training categories, matches will still be reported ... possibly with high confidence.</p><h2 id="data-sources" tabindex="-1">Data sources</h2><p><a href="http://imagenet.stanford.edu?v=1.0.21" rel="noopener" target="_blank">ImageNet</a> hosts a collection of over 14 million labeled images (with nouns) that can be used as input to deep learning models. It is maintained by Stanford University.</p><h2 id="image-recognition" tabindex="-1">Image recognition</h2><p>When deep learning is used for image recognition, images used for training and images used for recognition are converted to one-dimensional arrays of pixel values represented by a <code>torch.Tensor</code> object. Typically the images are fairly small, to avoid excessively long training times. The output is a set of predicted result classes that each have a human-readable label and a score, sorted from most to least likely match.</p><h2 id="conventions" tabindex="-1">Conventions</h2><table><thead><tr><th>Variable Name Suffix</th><th>Type</th></tr></thead><tbody><tr><td><code>_a</code></td><td>NumPy array</td></tr><tr><td><code>_g</code></td><td>GPU memory</td></tr><tr><td><code>_t</code></td><td>tensor</td></tr></tbody></table><h2 id="generative-adversarial-network-(gan)" tabindex="-1">Generative Adversarial Network (GAN)</h2><p>From <a href="https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/?v=1.0.21" rel="noopener" target="_blank">Machine Learning Mastery</a>, &quot;Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.&quot;</p><p>A &quot;generator network&quot; produced to output such as images. A &quot;discriminator network&quot; evaluates these to determine if they appear to be legitimate. Personally I don't find this use case to be particularly interesting.</p><h2 id="scene-classification" tabindex="-1">Scene Classification</h2><p>The NeuralTalk2 model is a example of a neural network that can describe what is sees in a photographed scene. The model has two parts. The first part learns to generate a numerical description of a scene. The second part takes these numerical descriptions as input and generates a sentence that describes the scene. Image/caption pairs are used to train this model.</p><p>The neural network for the second part is recurrent. It generates each of the words that will be present in the final sentence using separate passes through its network. Each word generated is dependent on the previously generated words.</p><p>Such models are interesting, but they their results are not yet consistent enough for real-world usage.</p><p>An implementation can be found in the GitHub repository for <a href="https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch?v=1.0.21" rel="noopener" target="_blank">deep-learning-with-pytorch/ImageCaptioning.pytorch</a> Note that this requires a GPU.</p><h2 id="torch-hub" tabindex="-1">Torch Hub</h2><p><a href="https://pytorch.org/hub/?v=1.0.21" rel="noopener" target="_blank">Torch Hub</a> is a web site to &quot;discover and publish models to a pre-trained model repository designed for research exploration.&quot; These models can be accessed through a PyTorch interface. Each repository must contain a <code>hubconf.py</code> file that describes the model.</p><p>One way to find these repositories is to search GitHub for repositories that contain a <code>hubconf.py</code> file.</p><p>For example, to obtain the resnet101 model from Torch Hub:</p><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> torch <span class="token keyword">import</span> hub<br><br>model <span class="token operator">=</span> hub<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'pytorch/vision:master'</span><span class="token punctuation">,</span> <span class="token string">'resnet101'</span><span class="token punctuation">,</span> pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><p>Add code similar to what was shown earlier to use this model for evaluating images.</p><p>Before running the code, install a compatible version of torchvision. Version 0.8.1 worked for me and can be installed with <code>pip install torchvision==0.8.1</code>.</p></article>